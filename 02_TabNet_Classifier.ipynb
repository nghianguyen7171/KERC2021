{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0      LALV\n1      HALV\n2      HALV\n3      LALV\n4      LALV\n       ... \n685    LAHV\n686    LAHV\n687    HALV\n688    LAHV\n689    LAHV\nName: label, Length: 690, dtype: object"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Import data\n",
    "train = pd.read_csv('/home/nghia/PR_LAB/KERC_Chall/kerc2021/KERC21Dataset/KERC21Dataset/train_Tab.csv')\n",
    "train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "0      3\n1      1\n2      1\n3      3\n4      3\n      ..\n685    2\n686    2\n687    1\n688    2\n689    2\nName: label, Length: 690, dtype: int64"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "nunique = train.nunique()\n",
    "types = train.dtypes\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims = {}\n",
    "\n",
    "for col in train.columns:\n",
    "    l_enc = LabelEncoder()\n",
    "    train[col] = l_enc.fit_transform(train[col].values)\n",
    "    categorical_columns.append(col)\n",
    "    categorical_dims[col] = len(l_enc.classes_)\n",
    "\n",
    "train['label']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316 316\n"
     ]
    }
   ],
   "source": [
    "fea_list = [x for x in train.columns.to_list() if x not in [\"ID\", \"label\"]]\n",
    "features = [col for col in train[fea_list]]\n",
    "cat_idxs = [i for i, f in enumerate(features) if f in categorical_columns]\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n",
    "print(len(cat_dims), len(cat_idxs))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n"
     ]
    }
   ],
   "source": [
    "#Tabnet classifier\n",
    "clf = TabNetClassifier(cat_idxs= cat_idxs,\n",
    "                       cat_dims= cat_dims,\n",
    "                       cat_emb_dim=1,\n",
    "                       optimizer_fn=torch.optim.Adam,\n",
    "                       optimizer_params=dict(lr=2e-2),\n",
    "                       scheduler_params={\"step_size\":50,\n",
    "                                         \"gamma\":0.9},\n",
    "                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                       mask_type= \"sparsemax\", #'entmax',\n",
    "                       #device_name='cpu'\n",
    "                       )\n",
    "X_train = train[features].values\n",
    "y_train = train[\"label\"].values\n",
    "\n",
    "max_epochs = 100 if not os.getenv(\"CI\", False) else 2\n",
    "#clf.fit(\n",
    " #   X_train, y_train, #X_train=X_train, y_train=y_train,\n",
    "    #eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    #eval_name=['train', 'valid'],\n",
    "    #eval_metric=['auc'],\n",
    "  #  max_epochs=max_epochs, patience=20,\n",
    "   # batch_size=1024, virtual_batch_size=128,\n",
    "   # num_workers=0,\n",
    "    #weights=1,\n",
    "    #drop_last=False\n",
    "#)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "val = pd.read_csv('/home/nghia/PR_LAB/KERC_Chall/kerc2021/KERC21Dataset/KERC21Dataset/val_Tab.csv')\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "nunique = val.nunique()\n",
    "types = val.dtypes\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims = {}\n",
    "\n",
    "for col in val.columns:\n",
    "    l_enc = LabelEncoder()\n",
    "    val[col] = l_enc.fit_transform(val[col].values)\n",
    "    categorical_columns.append(col)\n",
    "    categorical_dims[col] = len(l_enc.classes_)\n",
    "\n",
    "fea_val_list = [x for x in val.columns.to_list() if x not in [\"ID\"]]\n",
    "features = [col for col in val[fea_val_list]]\n",
    "cat_idxs = [i for i, f in enumerate(features) if f in categorical_columns]\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316 316\n"
     ]
    }
   ],
   "source": [
    "print(len(cat_dims), len(cat_idxs))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "X_val = val[features].values\n",
    "#y_val = val[\"label\"].values\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 3, 3, 1, 1, 0, 2, 1, 0, 2, 3, 3, 3, 3, 2, 1, 3, 1, 3, 3, 3,\n       3, 3, 3, 1, 1, 3, 3, 0, 3, 1, 3, 1, 1, 3, 2, 1, 1, 3, 3, 2, 3, 3,\n       1, 1, 0, 1, 3, 1, 1, 1, 1, 0, 3, 3, 1, 3, 3, 1, 1, 1, 0, 1, 3, 3,\n       2, 3, 2, 1, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 1, 1, 1, 3,\n       1, 3, 3, 1, 3, 1, 3, 1, 3, 3, 1, 3, 3, 3, 1, 3, 1, 3, 1, 2, 0, 0,\n       3, 3, 2, 3, 3, 3, 1, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1,\n       1, 1, 3, 3, 3, 3, 2, 1, 3, 1, 1, 3, 1, 1, 3, 3, 3, 1, 1, 1, 3, 2,\n       1, 1, 3, 3, 1, 1, 1, 1, 1, 3, 0, 1, 3, 1, 3, 3, 3, 2, 1, 3, 1, 3,\n       1, 3, 3, 3, 3, 0, 3, 3, 3, 3, 1, 1, 1, 3, 3, 1, 3, 0, 3, 1, 3, 0,\n       1, 3, 2, 0, 3, 3, 3, 1, 3, 3, 3, 0, 1, 3, 0, 3, 3, 1, 0, 0, 3, 1,\n       3, 3, 3, 3, 1, 3, 3, 1, 1, 1, 1, 1, 1, 3])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_val)\n",
    "y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "test = pd.read_csv('/home/nghia/PR_LAB/KERC_Chall/kerc2021/KERC21Dataset/KERC21Dataset/test_Tab.csv')\n",
    "nunique = test.nunique()\n",
    "types = test.dtypes\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims = {}\n",
    "\n",
    "for col in test.columns:\n",
    "    l_enc = LabelEncoder()\n",
    "    test[col] = l_enc.fit_transform(test[col].values)\n",
    "    categorical_columns.append(col)\n",
    "    categorical_dims[col] = len(l_enc.classes_)\n",
    "\n",
    "fea_test_list = [x for x in test.columns.to_list() if x not in [\"ID\"]]\n",
    "features = [col for col in test[fea_test_list]]\n",
    "cat_idxs = [i for i, f in enumerate(features) if f in categorical_columns]\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([3, 1, 1, 3, 1, 3, 1, 1, 3, 1, 3, 1, 1, 3, 1, 1, 3, 3, 3, 1, 3, 1,\n       3, 3, 3, 3, 3, 3, 2, 1, 3, 1, 1, 1, 3, 3, 1, 2, 3, 2, 1, 2, 1, 1,\n       3, 3, 1, 1, 1, 1, 1, 3, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3,\n       2, 1, 0, 3, 3, 3, 3, 1, 3, 2, 1, 3, 3, 3, 3, 3, 0, 3, 3, 3, 1, 2,\n       1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 3, 3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3,\n       3, 3, 0, 3, 3, 3, 0, 3, 3, 3, 1, 2, 3, 3, 1, 3, 3, 1, 3, 1, 3, 3,\n       1, 3, 3, 3, 3, 1, 1, 2, 3, 2, 3, 3, 2, 3, 3, 3, 1, 3, 3, 3, 3, 3,\n       3, 3, 1, 1, 1, 1, 3, 3, 3, 1, 3, 1, 3, 3, 1, 3, 3, 1, 1, 3, 3, 3,\n       3, 1, 1, 1, 1, 2, 1, 3, 1, 1, 3, 1, 0, 1, 3, 1, 3, 2, 3, 0, 0, 3,\n       3, 3, 3, 3, 0, 3, 1, 3, 3, 1, 3, 1, 3, 3, 3, 1, 3, 1, 1, 1, 1, 3,\n       1, 1, 0, 3, 3, 3, 3, 3, 3, 3, 2, 1, 3, 2, 3, 1, 0, 1, 3, 3, 1, 3,\n       1, 3, 3, 1, 1, 1, 3, 1, 3, 2, 3, 3, 0, 1, 3, 3, 3, 1, 1, 3, 3, 3,\n       3, 2, 3, 3, 3, 3, 1, 3, 3, 3, 3, 0, 1, 1, 2, 1, 1, 3, 3, 3, 1, 1,\n       2, 3])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = test[features].values\n",
    "\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([3, 1, 1, 3, 3, 3, 3, 3, 1, 3, 3, 0, 3, 1, 3, 1, 1, 3, 1, 0, 0, 1,\n       1, 0, 0, 3, 1, 2, 1, 1, 1, 3, 1, 1, 3, 1, 1, 1, 3, 3, 0, 1, 3, 1,\n       3, 1, 3, 3, 3, 1, 2, 3, 0, 1, 0, 3, 3, 1, 3, 1, 2, 3, 3, 3, 1, 3,\n       3, 1, 3, 1, 2, 3, 3, 2, 1, 3, 1, 1, 2, 2, 0, 1, 3, 3, 1, 3, 2, 2,\n       1, 1, 0, 3, 1, 3, 1, 3, 1, 3, 3, 0, 2, 3, 1, 0, 3, 3, 3, 1, 0, 2,\n       1, 1, 1, 2, 3, 3, 1, 1, 3, 3, 1, 3, 2, 3, 3, 2, 1, 1, 1, 1, 3, 3,\n       1, 1, 3, 3, 1, 3, 2, 2, 0, 3, 3, 2, 3, 2, 0, 3, 1, 3, 1, 3, 3, 3,\n       3, 1, 1, 1, 1, 1, 2, 3, 3, 1, 2, 1, 3, 3, 3, 2, 1, 1, 3, 1, 0, 1,\n       3, 3, 3, 1, 1, 3, 1, 3, 3, 3, 3, 3, 2, 3, 1, 3, 1, 1, 3, 2, 2, 1,\n       1, 2, 1, 1, 1, 3, 3, 3, 2, 3, 3, 1, 1, 3, 1, 2, 0, 1, 3, 1, 3, 3,\n       1, 2, 3, 2, 1, 2, 3, 1, 1, 3, 1, 1, 0, 0, 2, 3, 1, 3, 1, 1, 3, 3,\n       2, 3, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 2, 3, 1, 1, 3, 3, 1,\n       0, 3, 1, 3, 2, 2, 3, 0, 3, 0, 0, 3, 0, 3, 1, 0, 1, 1, 3, 3, 0, 3,\n       0, 1, 3, 0, 3, 3, 1, 0, 3, 3, 1, 1, 1, 0, 3, 0, 3, 0, 3, 1, 1, 3,\n       3, 1, 3, 3, 3, 2, 1, 1, 1, 3, 2, 2, 1, 3, 3, 3, 0, 1, 3, 1, 1, 1,\n       1, 1, 3, 3, 1, 2, 2, 1, 0, 3, 3, 3, 1, 3, 3, 3, 1, 1, 3, 3, 3, 3,\n       3, 2, 3, 0, 3, 1, 2, 1, 3, 2, 3, 3, 1, 3, 3, 3, 2, 2, 1, 3, 2, 3,\n       1, 3, 1, 1, 2, 0, 2, 0, 1, 1, 0, 0, 1, 3, 3, 3, 3, 3, 1, 1, 3, 1,\n       3, 3, 2, 2, 3, 1, 3, 1, 3, 0, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 1, 0,\n       3, 3, 2, 0, 3, 0, 3, 2, 1, 1, 2, 3, 2, 1, 1, 1, 1, 1, 3, 2, 3, 2,\n       3, 1, 1, 1, 3, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3,\n       3, 1, 0, 3, 3, 1, 2, 1, 2, 1, 3, 3, 1, 1, 1, 1, 2, 3, 1, 3, 2, 3,\n       0, 0, 1, 1, 1, 3, 3, 1, 3, 1, 2, 3, 1, 1, 0, 3, 3, 1, 1, 3, 1, 2,\n       1, 1, 0, 3, 1, 1, 1, 1, 3, 1, 3, 3, 3, 0, 1, 3, 3, 3, 0, 2, 1, 3,\n       2, 1, 3, 1, 1, 1, 1, 1, 3, 2, 1, 1, 0, 3, 2, 1, 1, 1, 1, 0, 0, 0,\n       3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 1, 1, 0, 3, 3, 3, 2, 2, 0, 1, 1, 3,\n       1, 1, 0, 3, 1, 2, 1, 1, 2, 1, 1, 1, 3, 3, 3, 0, 3, 1, 1, 1, 3, 1,\n       2, 1, 2, 1, 2, 3, 3, 3, 1, 1, 3, 3, 1, 3, 3, 0, 1, 3, 3, 1, 0, 3,\n       1, 1, 2, 1, 3, 1, 3, 1, 3, 1, 2, 3, 3, 3, 2, 0, 2, 1, 3, 3, 3, 1,\n       0, 1, 3, 1, 2, 1, 0, 2, 1, 0, 1, 1, 1, 3, 1, 1, 1, 2, 2, 3, 3, 1,\n       1, 3, 1, 3, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 3, 3, 2, 2, 1, 1,\n       2, 3, 1, 2, 2, 1, 2, 2])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['HAHV', 'HALV', 'HALV', 'HAHV', 'HALV', 'HAHV', 'HALV', 'HALV',\n       'HAHV', 'HALV', 'HAHV', 'HALV', 'HALV', 'HAHV', 'HALV', 'HALV',\n       'HAHV', 'HAHV', 'HAHV', 'HALV', 'HAHV', 'HALV', 'HAHV', 'HAHV',\n       'HAHV', 'HAHV', 'HAHV', 'HAHV', 'LAHV', 'HALV', 'HAHV', 'HALV',\n       'HALV', 'HALV', 'HAHV', 'HAHV', 'HALV', 'LAHV', 'HAHV', 'LAHV',\n       'HALV', 'LAHV', 'HALV', 'HALV', 'HAHV', 'HAHV', 'HALV', 'HALV',\n       'HALV', 'HALV', 'HALV', 'HAHV', 'HAHV', 'HALV', 'HAHV', 'HALV',\n       'HAHV', 'HALV', 'HAHV', 'HAHV', 'HAHV', 'HAHV', 'HAHV', 'HALV',\n       'HAHV', 'HAHV', 'LAHV', 'HALV', 'LALV', 'HAHV', 'HAHV', 'HAHV',\n       'HAHV', 'HALV', 'HAHV', 'LAHV', 'HALV', 'HAHV', 'HAHV', 'HAHV',\n       'HAHV', 'HAHV', 'LALV', 'HAHV', 'HAHV', 'HAHV', 'HALV', 'LAHV',\n       'HALV', 'HALV', 'HALV', 'HALV', 'HALV', 'HAHV', 'HALV', 'HAHV',\n       'HALV', 'HALV', 'HAHV', 'HAHV', 'HALV', 'HAHV', 'HAHV', 'HAHV',\n       'HALV', 'HAHV', 'HAHV', 'HALV', 'HALV', 'HAHV', 'HAHV', 'HAHV',\n       'LALV', 'HAHV', 'HAHV', 'HAHV', 'LALV', 'HAHV', 'HAHV', 'HAHV',\n       'HALV', 'LAHV', 'HAHV', 'HAHV', 'HALV', 'HAHV', 'HAHV', 'HALV',\n       'HAHV', 'HALV', 'HAHV', 'HAHV', 'HALV', 'HAHV', 'HAHV', 'HAHV',\n       'HAHV', 'HALV', 'HALV', 'LAHV', 'HAHV', 'LAHV', 'HAHV', 'HAHV',\n       'LAHV', 'HAHV', 'HAHV', 'HAHV', 'HALV', 'HAHV', 'HAHV', 'HAHV',\n       'HAHV', 'HAHV', 'HAHV', 'HAHV', 'HALV', 'HALV', 'HALV', 'HALV',\n       'HAHV', 'HAHV', 'HAHV', 'HALV', 'HAHV', 'HALV', 'HAHV', 'HAHV',\n       'HALV', 'HAHV', 'HAHV', 'HALV', 'HALV', 'HAHV', 'HAHV', 'HAHV',\n       'HAHV', 'HALV', 'HALV', 'HALV', 'HALV', 'LAHV', 'HALV', 'HAHV',\n       'HALV', 'HALV', 'HAHV', 'HALV', 'LALV', 'HALV', 'HAHV', 'HALV',\n       'HAHV', 'LAHV', 'HAHV', 'LALV', 'LALV', 'HAHV', 'HAHV', 'HAHV',\n       'HAHV', 'HAHV', 'LALV', 'HAHV', 'HALV', 'HAHV', 'HAHV', 'HALV',\n       'HAHV', 'HALV', 'HAHV', 'HAHV', 'HAHV', 'HALV', 'HAHV', 'HALV',\n       'HALV', 'HALV', 'HALV', 'HAHV', 'HALV', 'HALV', 'LALV', 'HAHV',\n       'HAHV', 'HAHV', 'HAHV', 'HAHV', 'HAHV', 'HAHV', 'LAHV', 'HALV',\n       'HAHV', 'LAHV', 'HAHV', 'HALV', 'LALV', 'HALV', 'HAHV', 'HAHV',\n       'HALV', 'HAHV', 'HALV', 'HAHV', 'HAHV', 'HALV', 'HALV', 'HALV',\n       'HAHV', 'HALV', 'HAHV', 'LAHV', 'HAHV', 'HAHV', 'LALV', 'HALV',\n       'HAHV', 'HAHV', 'HAHV', 'HALV', 'HALV', 'HAHV', 'HAHV', 'HAHV',\n       'HAHV', 'LAHV', 'HAHV', 'HAHV', 'HAHV', 'HAHV', 'HALV', 'HAHV',\n       'HAHV', 'HAHV', 'HAHV', 'LALV', 'HALV', 'HALV', 'LAHV', 'HALV',\n       'HALV', 'HAHV', 'HAHV', 'HAHV', 'HALV', 'HALV', 'LAHV', 'HAHV'],\n      dtype='<U4')"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_names = np.array([\"LALV\", \"HALV\", \"LAHV\", \"HAHV\"])\n",
    "dict_names[y_pred_test]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(X_train, y_train, random_state=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 1.25786 |  0:00:00s\n",
      "epoch 1  | loss: 0.69227 |  0:00:00s\n",
      "epoch 2  | loss: 0.50939 |  0:00:00s\n",
      "epoch 3  | loss: 0.57487 |  0:00:00s\n",
      "epoch 4  | loss: 0.41998 |  0:00:00s\n",
      "epoch 5  | loss: 0.41619 |  0:00:00s\n",
      "epoch 6  | loss: 0.40817 |  0:00:00s\n",
      "epoch 7  | loss: 0.22652 |  0:00:00s\n",
      "epoch 8  | loss: 0.31879 |  0:00:00s\n",
      "epoch 9  | loss: 0.23128 |  0:00:00s\n",
      "epoch 10 | loss: 0.26739 |  0:00:00s\n",
      "epoch 11 | loss: 0.15415 |  0:00:00s\n",
      "epoch 12 | loss: 0.26912 |  0:00:00s\n",
      "epoch 13 | loss: 0.25171 |  0:00:01s\n",
      "epoch 14 | loss: 0.20351 |  0:00:01s\n",
      "epoch 15 | loss: 0.18181 |  0:00:01s\n",
      "epoch 16 | loss: 0.20005 |  0:00:01s\n",
      "epoch 17 | loss: 0.14656 |  0:00:01s\n",
      "epoch 18 | loss: 0.13071 |  0:00:01s\n",
      "epoch 19 | loss: 0.17443 |  0:00:01s\n",
      "epoch 20 | loss: 0.1118  |  0:00:01s\n",
      "epoch 21 | loss: 0.09227 |  0:00:01s\n",
      "epoch 22 | loss: 0.13599 |  0:00:01s\n",
      "epoch 23 | loss: 0.10216 |  0:00:01s\n",
      "epoch 24 | loss: 0.08621 |  0:00:01s\n",
      "epoch 25 | loss: 0.10061 |  0:00:01s\n",
      "epoch 26 | loss: 0.07817 |  0:00:01s\n",
      "epoch 27 | loss: 0.06108 |  0:00:01s\n",
      "epoch 28 | loss: 0.07109 |  0:00:02s\n",
      "epoch 29 | loss: 0.0553  |  0:00:02s\n",
      "epoch 30 | loss: 0.04088 |  0:00:02s\n",
      "epoch 31 | loss: 0.05556 |  0:00:02s\n",
      "epoch 32 | loss: 0.03929 |  0:00:02s\n",
      "epoch 33 | loss: 0.03969 |  0:00:02s\n",
      "epoch 34 | loss: 0.02486 |  0:00:02s\n",
      "epoch 35 | loss: 0.03735 |  0:00:02s\n",
      "epoch 36 | loss: 0.04154 |  0:00:02s\n",
      "epoch 37 | loss: 0.03301 |  0:00:02s\n",
      "epoch 38 | loss: 0.02739 |  0:00:02s\n",
      "epoch 39 | loss: 0.02233 |  0:00:02s\n",
      "epoch 40 | loss: 0.03375 |  0:00:02s\n",
      "epoch 41 | loss: 0.02024 |  0:00:02s\n",
      "epoch 42 | loss: 0.02841 |  0:00:02s\n",
      "epoch 43 | loss: 0.01858 |  0:00:03s\n",
      "epoch 44 | loss: 0.04066 |  0:00:03s\n",
      "epoch 45 | loss: 0.02421 |  0:00:03s\n",
      "epoch 46 | loss: 0.01694 |  0:00:03s\n",
      "epoch 47 | loss: 0.01973 |  0:00:03s\n",
      "epoch 48 | loss: 0.02926 |  0:00:03s\n",
      "epoch 49 | loss: 0.01034 |  0:00:03s\n",
      "epoch 50 | loss: 0.01305 |  0:00:03s\n",
      "epoch 51 | loss: 0.01004 |  0:00:03s\n",
      "epoch 52 | loss: 0.02377 |  0:00:03s\n",
      "epoch 53 | loss: 0.02485 |  0:00:03s\n",
      "epoch 54 | loss: 0.02032 |  0:00:03s\n",
      "epoch 55 | loss: 0.0165  |  0:00:03s\n",
      "epoch 56 | loss: 0.01397 |  0:00:03s\n",
      "epoch 57 | loss: 0.01071 |  0:00:03s\n",
      "epoch 58 | loss: 0.01549 |  0:00:04s\n",
      "epoch 59 | loss: 0.00737 |  0:00:04s\n",
      "epoch 60 | loss: 0.00429 |  0:00:04s\n",
      "epoch 61 | loss: 0.00884 |  0:00:04s\n",
      "epoch 62 | loss: 0.02023 |  0:00:04s\n",
      "epoch 63 | loss: 0.00511 |  0:00:04s\n",
      "epoch 64 | loss: 0.00709 |  0:00:04s\n",
      "epoch 65 | loss: 0.00569 |  0:00:04s\n",
      "epoch 66 | loss: 0.00777 |  0:00:04s\n",
      "epoch 67 | loss: 0.03444 |  0:00:04s\n",
      "epoch 68 | loss: 0.02483 |  0:00:04s\n",
      "epoch 69 | loss: 0.007   |  0:00:04s\n",
      "epoch 70 | loss: 0.00516 |  0:00:04s\n",
      "epoch 71 | loss: 0.03088 |  0:00:04s\n",
      "epoch 72 | loss: 0.00764 |  0:00:04s\n",
      "epoch 73 | loss: 0.01201 |  0:00:05s\n",
      "epoch 74 | loss: 0.00688 |  0:00:05s\n",
      "epoch 75 | loss: 0.04275 |  0:00:05s\n",
      "epoch 76 | loss: 0.01329 |  0:00:05s\n",
      "epoch 77 | loss: 0.03756 |  0:00:05s\n",
      "epoch 78 | loss: 0.03078 |  0:00:05s\n",
      "epoch 79 | loss: 0.02192 |  0:00:05s\n",
      "epoch 80 | loss: 0.01705 |  0:00:05s\n",
      "epoch 81 | loss: 0.01662 |  0:00:05s\n",
      "epoch 82 | loss: 0.01862 |  0:00:05s\n",
      "epoch 83 | loss: 0.01757 |  0:00:05s\n",
      "epoch 84 | loss: 0.00687 |  0:00:05s\n",
      "epoch 85 | loss: 0.02019 |  0:00:05s\n",
      "epoch 86 | loss: 0.00941 |  0:00:05s\n",
      "epoch 87 | loss: 0.01208 |  0:00:05s\n",
      "epoch 88 | loss: 0.01424 |  0:00:06s\n",
      "epoch 89 | loss: 0.00898 |  0:00:06s\n",
      "epoch 90 | loss: 0.01178 |  0:00:06s\n",
      "epoch 91 | loss: 0.03367 |  0:00:06s\n",
      "epoch 92 | loss: 0.00338 |  0:00:06s\n",
      "epoch 93 | loss: 0.00291 |  0:00:06s\n",
      "epoch 94 | loss: 0.01518 |  0:00:06s\n",
      "epoch 95 | loss: 0.00524 |  0:00:06s\n",
      "epoch 96 | loss: 0.01792 |  0:00:06s\n",
      "epoch 97 | loss: 0.03758 |  0:00:06s\n",
      "epoch 98 | loss: 0.00582 |  0:00:06s\n",
      "epoch 99 | loss: 0.02697 |  0:00:06s\n"
     ]
    }
   ],
   "source": [
    "clf.fit(\n",
    "    X_train_sub, y_train_sub, #X_train=X_train, y_train=y_train,\n",
    "    #eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    #eval_name=['train', 'valid'],\n",
    "    #eval_metric=['auc'],\n",
    "    max_epochs=max_epochs, patience=20,\n",
    "    batch_size=1024, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    weights=1,\n",
    "    drop_last=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "0.7745664739884393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_pred_sub = clf.predict(X_test_sub)\n",
    "\n",
    "print(f\"Classification report:\\n\"\n",
    "f\"{metrics.f1_score(y_test_sub, y_pred_sub, average='micro')}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(288, 317)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.0089  | train_accuracy: 0.94203 | valid_accuracy: 0.77457 |  0:00:00s\n",
      "epoch 1  | loss: 0.01648 | train_accuracy: 0.93768 | valid_accuracy: 0.76301 |  0:00:00s\n",
      "epoch 2  | loss: 0.02852 | train_accuracy: 0.94203 | valid_accuracy: 0.78035 |  0:00:00s\n",
      "epoch 3  | loss: 0.07128 | train_accuracy: 0.93623 | valid_accuracy: 0.75145 |  0:00:00s\n",
      "epoch 4  | loss: 0.01788 | train_accuracy: 0.93333 | valid_accuracy: 0.74566 |  0:00:00s\n",
      "epoch 5  | loss: 0.06267 | train_accuracy: 0.93188 | valid_accuracy: 0.73988 |  0:00:00s\n",
      "epoch 6  | loss: 0.02966 | train_accuracy: 0.92899 | valid_accuracy: 0.74566 |  0:00:01s\n",
      "epoch 7  | loss: 0.031   | train_accuracy: 0.92754 | valid_accuracy: 0.73988 |  0:00:01s\n",
      "epoch 8  | loss: 0.03514 | train_accuracy: 0.92899 | valid_accuracy: 0.73988 |  0:00:01s\n",
      "epoch 9  | loss: 0.02431 | train_accuracy: 0.92609 | valid_accuracy: 0.71676 |  0:00:01s\n",
      "epoch 10 | loss: 0.03672 | train_accuracy: 0.92754 | valid_accuracy: 0.71676 |  0:00:01s\n",
      "epoch 11 | loss: 0.02078 | train_accuracy: 0.93188 | valid_accuracy: 0.7341  |  0:00:01s\n",
      "epoch 12 | loss: 0.02328 | train_accuracy: 0.92899 | valid_accuracy: 0.72832 |  0:00:01s\n",
      "epoch 13 | loss: 0.02969 | train_accuracy: 0.93188 | valid_accuracy: 0.73988 |  0:00:01s\n",
      "epoch 14 | loss: 0.02402 | train_accuracy: 0.93188 | valid_accuracy: 0.74566 |  0:00:02s\n",
      "epoch 15 | loss: 0.0438  | train_accuracy: 0.92609 | valid_accuracy: 0.72254 |  0:00:02s\n",
      "epoch 16 | loss: 0.0212  | train_accuracy: 0.92464 | valid_accuracy: 0.71676 |  0:00:02s\n",
      "epoch 17 | loss: 0.03129 | train_accuracy: 0.92609 | valid_accuracy: 0.71676 |  0:00:02s\n",
      "epoch 18 | loss: 0.01529 | train_accuracy: 0.92754 | valid_accuracy: 0.72254 |  0:00:02s\n",
      "epoch 19 | loss: 0.00971 | train_accuracy: 0.93043 | valid_accuracy: 0.72254 |  0:00:02s\n",
      "epoch 20 | loss: 0.03103 | train_accuracy: 0.93043 | valid_accuracy: 0.72254 |  0:00:02s\n",
      "epoch 21 | loss: 0.00975 | train_accuracy: 0.92899 | valid_accuracy: 0.72832 |  0:00:02s\n",
      "epoch 22 | loss: 0.01054 | train_accuracy: 0.92899 | valid_accuracy: 0.72832 |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 2 and best_valid_accuracy = 0.78035\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "#import tenso\n",
    "#callback = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "clf.fit(\n",
    "    X_train_sub, y_train_sub, #X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test_sub, y_test_sub)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=max_epochs, patience=20,\n",
    "    batch_size=1024, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    weights=1,\n",
    "    drop_last=False,\n",
    "    #callbacks=[callback]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "           ID         0         1         2         3         4         5  \\\n0    22Nj7cPW -0.019885  0.046958  0.733678  0.618324  1.042693  1.461745   \n1    2A9QeqgD -0.042184  0.064785  0.963826  0.708307  1.132032  1.538334   \n2    2AwkKhSz  0.057305  0.031962  0.676590  0.631667  1.168806  1.613567   \n3    2SEMzqof -0.072864  0.087234  1.031330  0.284738  0.542283  0.845110   \n4    2Sq686TD  0.000000  0.000000  0.000000  0.103822  0.007811 -0.004460   \n..        ...       ...       ...       ...       ...       ...       ...   \n283  KiSPDRg8  0.087915  0.021209  0.637186  0.336705  0.682841  1.051483   \n284  bKHxJjeg  0.001115  0.001182  0.009039  0.003976  0.007124  0.003039   \n285  3PNiPgms -0.005370  0.000922 -0.002394 -0.003417 -0.007401 -0.009789   \n286  jyUDNewE  0.000000  0.000000  0.000000  1.520619  0.104700 -0.201841   \n287  gnSced5m -0.047915  0.045915  0.655776  0.500756  0.996164  1.355669   \n\n            6         7         8  ...       306       307       308  \\\n0    1.664820  1.746297 -8.286156  ... -0.610064  3.167050  3.720327   \n1    1.881786  1.995223 -6.215798  ... -0.038016 -5.161601 -1.881224   \n2    1.975880  2.019866 -6.183857  ...  0.546570 -4.731420 -1.383980   \n3    1.182093  1.357307 -7.952219  ... -0.393127 -1.146166  2.782749   \n4   -0.006873 -0.003280 -0.005154  ...  0.740072 -5.031872 -4.574648   \n..        ...       ...       ...  ...       ...       ...       ...   \n283  1.254413  1.612583 -8.117909  ...  0.307423 -0.472953 -5.095288   \n284  0.004125  0.000028  0.000154  ...  1.761269 -5.876279 -5.192789   \n285 -0.008063  0.001677 -0.008518  ...  1.288340 -7.018189 -6.288341   \n286  0.049185  0.739341  0.558004  ...  1.878504  1.319542 -2.443841   \n287  1.452034  1.758237 -6.173355  ...  3.614004  5.176155  5.074651   \n\n          309       310       311       312       313        314       315  \n0    1.201704 -0.602058  0.000000  0.000000  0.000000   3.990296  0.601049  \n1    0.272850  0.268077  0.000000  0.000000  0.000000   0.219902  0.014584  \n2    0.791501  0.495659  0.000000  0.000000  0.000000   0.288689  0.018951  \n3    1.507315  0.300281 -0.161241  1.992280  0.663413  -0.173976 -0.694309  \n4   -5.549634 -5.120717 -2.138377  2.276388 -4.976772  -4.874982  1.915179  \n..        ...       ...       ...       ...       ...        ...       ...  \n283 -2.226338  0.168159 -0.318635  3.054167  1.812393   1.226686  0.554353  \n284 -5.600039 -1.889896 -5.850311  2.347711 -4.213875  -3.215955  4.853327  \n285 -6.838863 -5.384285 -3.139098  1.919908 -5.632252  -5.086109  1.778868  \n286  1.110149  2.935334  2.289370 -2.546097  0.957361   2.796655  2.283839  \n287  4.882742  4.280547  0.000000  0.000000  0.000000  10.462851  0.828578  \n\n[522 rows x 317 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>...</th>\n      <th>306</th>\n      <th>307</th>\n      <th>308</th>\n      <th>309</th>\n      <th>310</th>\n      <th>311</th>\n      <th>312</th>\n      <th>313</th>\n      <th>314</th>\n      <th>315</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22Nj7cPW</td>\n      <td>-0.019885</td>\n      <td>0.046958</td>\n      <td>0.733678</td>\n      <td>0.618324</td>\n      <td>1.042693</td>\n      <td>1.461745</td>\n      <td>1.664820</td>\n      <td>1.746297</td>\n      <td>-8.286156</td>\n      <td>...</td>\n      <td>-0.610064</td>\n      <td>3.167050</td>\n      <td>3.720327</td>\n      <td>1.201704</td>\n      <td>-0.602058</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>3.990296</td>\n      <td>0.601049</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2A9QeqgD</td>\n      <td>-0.042184</td>\n      <td>0.064785</td>\n      <td>0.963826</td>\n      <td>0.708307</td>\n      <td>1.132032</td>\n      <td>1.538334</td>\n      <td>1.881786</td>\n      <td>1.995223</td>\n      <td>-6.215798</td>\n      <td>...</td>\n      <td>-0.038016</td>\n      <td>-5.161601</td>\n      <td>-1.881224</td>\n      <td>0.272850</td>\n      <td>0.268077</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.219902</td>\n      <td>0.014584</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2AwkKhSz</td>\n      <td>0.057305</td>\n      <td>0.031962</td>\n      <td>0.676590</td>\n      <td>0.631667</td>\n      <td>1.168806</td>\n      <td>1.613567</td>\n      <td>1.975880</td>\n      <td>2.019866</td>\n      <td>-6.183857</td>\n      <td>...</td>\n      <td>0.546570</td>\n      <td>-4.731420</td>\n      <td>-1.383980</td>\n      <td>0.791501</td>\n      <td>0.495659</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.288689</td>\n      <td>0.018951</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2SEMzqof</td>\n      <td>-0.072864</td>\n      <td>0.087234</td>\n      <td>1.031330</td>\n      <td>0.284738</td>\n      <td>0.542283</td>\n      <td>0.845110</td>\n      <td>1.182093</td>\n      <td>1.357307</td>\n      <td>-7.952219</td>\n      <td>...</td>\n      <td>-0.393127</td>\n      <td>-1.146166</td>\n      <td>2.782749</td>\n      <td>1.507315</td>\n      <td>0.300281</td>\n      <td>-0.161241</td>\n      <td>1.992280</td>\n      <td>0.663413</td>\n      <td>-0.173976</td>\n      <td>-0.694309</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2Sq686TD</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.103822</td>\n      <td>0.007811</td>\n      <td>-0.004460</td>\n      <td>-0.006873</td>\n      <td>-0.003280</td>\n      <td>-0.005154</td>\n      <td>...</td>\n      <td>0.740072</td>\n      <td>-5.031872</td>\n      <td>-4.574648</td>\n      <td>-5.549634</td>\n      <td>-5.120717</td>\n      <td>-2.138377</td>\n      <td>2.276388</td>\n      <td>-4.976772</td>\n      <td>-4.874982</td>\n      <td>1.915179</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>283</th>\n      <td>KiSPDRg8</td>\n      <td>0.087915</td>\n      <td>0.021209</td>\n      <td>0.637186</td>\n      <td>0.336705</td>\n      <td>0.682841</td>\n      <td>1.051483</td>\n      <td>1.254413</td>\n      <td>1.612583</td>\n      <td>-8.117909</td>\n      <td>...</td>\n      <td>0.307423</td>\n      <td>-0.472953</td>\n      <td>-5.095288</td>\n      <td>-2.226338</td>\n      <td>0.168159</td>\n      <td>-0.318635</td>\n      <td>3.054167</td>\n      <td>1.812393</td>\n      <td>1.226686</td>\n      <td>0.554353</td>\n    </tr>\n    <tr>\n      <th>284</th>\n      <td>bKHxJjeg</td>\n      <td>0.001115</td>\n      <td>0.001182</td>\n      <td>0.009039</td>\n      <td>0.003976</td>\n      <td>0.007124</td>\n      <td>0.003039</td>\n      <td>0.004125</td>\n      <td>0.000028</td>\n      <td>0.000154</td>\n      <td>...</td>\n      <td>1.761269</td>\n      <td>-5.876279</td>\n      <td>-5.192789</td>\n      <td>-5.600039</td>\n      <td>-1.889896</td>\n      <td>-5.850311</td>\n      <td>2.347711</td>\n      <td>-4.213875</td>\n      <td>-3.215955</td>\n      <td>4.853327</td>\n    </tr>\n    <tr>\n      <th>285</th>\n      <td>3PNiPgms</td>\n      <td>-0.005370</td>\n      <td>0.000922</td>\n      <td>-0.002394</td>\n      <td>-0.003417</td>\n      <td>-0.007401</td>\n      <td>-0.009789</td>\n      <td>-0.008063</td>\n      <td>0.001677</td>\n      <td>-0.008518</td>\n      <td>...</td>\n      <td>1.288340</td>\n      <td>-7.018189</td>\n      <td>-6.288341</td>\n      <td>-6.838863</td>\n      <td>-5.384285</td>\n      <td>-3.139098</td>\n      <td>1.919908</td>\n      <td>-5.632252</td>\n      <td>-5.086109</td>\n      <td>1.778868</td>\n    </tr>\n    <tr>\n      <th>286</th>\n      <td>jyUDNewE</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.520619</td>\n      <td>0.104700</td>\n      <td>-0.201841</td>\n      <td>0.049185</td>\n      <td>0.739341</td>\n      <td>0.558004</td>\n      <td>...</td>\n      <td>1.878504</td>\n      <td>1.319542</td>\n      <td>-2.443841</td>\n      <td>1.110149</td>\n      <td>2.935334</td>\n      <td>2.289370</td>\n      <td>-2.546097</td>\n      <td>0.957361</td>\n      <td>2.796655</td>\n      <td>2.283839</td>\n    </tr>\n    <tr>\n      <th>287</th>\n      <td>gnSced5m</td>\n      <td>-0.047915</td>\n      <td>0.045915</td>\n      <td>0.655776</td>\n      <td>0.500756</td>\n      <td>0.996164</td>\n      <td>1.355669</td>\n      <td>1.452034</td>\n      <td>1.758237</td>\n      <td>-6.173355</td>\n      <td>...</td>\n      <td>3.614004</td>\n      <td>5.176155</td>\n      <td>5.074651</td>\n      <td>4.882742</td>\n      <td>4.280547</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>10.462851</td>\n      <td>0.828578</td>\n    </tr>\n  </tbody>\n</table>\n<p>522 rows × 317 columns</p>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = pd.read_csv('/home/nghia/PR_LAB/KERC_Chall/kerc2021/KERC21Dataset/KERC21Dataset/val_Tab.csv')\n",
    "test = pd.read_csv('/home/nghia/PR_LAB/KERC_Chall/kerc2021/KERC21Dataset/KERC21Dataset/test_Tab.csv')\n",
    "\n",
    "submission_df = pd.concat([val, test])\n",
    "submission_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(316, 316)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nunique = submission_df.nunique()\n",
    "types = submission_df.dtypes\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims = {}\n",
    "\n",
    "for col in submission_df.columns:\n",
    "    l_enc = LabelEncoder()\n",
    "    submission_df[col] = l_enc.fit_transform(submission_df[col].values)\n",
    "    categorical_columns.append(col)\n",
    "    categorical_dims[col] = len(l_enc.classes_)\n",
    "\n",
    "fea_test_list = [x for x in submission_df.columns.to_list() if x not in [\"ID\"]]\n",
    "features = [col for col in submission_df[fea_test_list]]\n",
    "cat_idxs = [i for i, f in enumerate(features) if f in categorical_columns]\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n",
    "len(cat_dims), len(cat_idxs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 3, 1, 1, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3,\n       1, 3, 1, 1, 2, 2, 3, 1, 3, 1, 0, 2, 1, 0, 1, 3, 3, 3, 2, 3, 0, 0,\n       3, 3, 1, 0, 2, 1, 2, 3, 3, 3, 0, 3, 3, 1, 3, 3, 3, 3, 1, 1, 1, 0,\n       1, 3, 1, 2, 3, 3, 1, 1, 3, 1, 1, 1, 2, 1, 3, 3, 3, 2, 3, 3, 3, 1,\n       1, 0, 1, 3, 0, 3, 3, 3, 3, 3, 2, 2, 1, 3, 2, 2, 0, 0, 1, 2, 1, 1,\n       3, 0, 3, 0, 2, 2, 1, 1, 1, 1, 3, 3, 1, 0, 3, 0, 3, 1, 0, 0, 0, 3,\n       3, 3, 3, 1, 3, 1, 1, 3, 3, 2, 2, 1, 2, 3, 3, 1, 1, 0, 0, 1, 1, 3,\n       3, 3, 1, 3, 2, 1, 2, 3, 3, 3, 1, 3, 1, 1, 1, 3, 3, 1, 1, 3, 1, 3,\n       1, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 0, 3, 1, 1, 3, 3, 0, 3, 2, 1, 1,\n       3, 3, 3, 3, 1, 3, 2, 0, 0, 3, 3, 3, 3, 0, 0, 1, 3, 3, 1, 3, 1, 0,\n       3, 2, 3, 2, 3, 3, 3, 3, 2, 1, 1, 3, 3, 1, 3, 1, 1, 1, 0, 3, 3, 3,\n       3, 3, 1, 3, 3, 1, 2, 0, 3, 1, 1, 2, 1, 3, 1, 1, 1, 2, 2, 2, 3, 2,\n       3, 2, 1, 0, 1, 2, 2, 3, 3, 0, 3, 0, 3, 3, 2, 1, 0, 3, 2, 0, 3, 3,\n       3, 1, 3, 1, 3, 2, 0, 3, 2, 0, 3, 1, 3, 2, 1, 0, 1, 2, 2, 1, 3, 1,\n       3, 1, 1, 1, 1, 0, 3, 3, 1, 2, 1, 3, 3, 2, 1, 3, 0, 2, 3, 1, 1, 3,\n       0, 3, 2, 1, 1, 3, 2, 0, 3, 3, 1, 1, 3, 1, 3, 1, 3, 1, 3, 1, 1, 3,\n       3, 2, 3, 3, 0, 3, 3, 3, 3, 2, 0, 1, 2, 1, 3, 2, 3, 1, 3, 3, 3, 1,\n       2, 0, 3, 1, 2, 1, 2, 2, 1, 2, 0, 2, 3, 1, 2, 3, 3, 3, 3, 3, 3, 2,\n       2, 1, 3, 3, 3, 1, 1, 3, 1, 3, 1, 3, 3, 3, 1, 3, 0, 3, 3, 3, 0, 1,\n       3, 1, 3, 2, 3, 1, 1, 3, 1, 3, 3, 1, 3, 3, 3, 1, 1, 2, 1, 1, 0, 0,\n       3, 1, 1, 1, 2, 3, 1, 3, 1, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 2, 2, 1,\n       0, 3, 3, 3, 1, 3, 0, 2, 3, 3, 3, 3, 0, 3, 1, 1, 0, 1, 0, 1, 2, 2,\n       3, 2, 3, 1, 1, 1, 1, 3, 3, 1, 1, 3, 3, 2, 3, 3, 2, 3, 1, 3, 0, 3,\n       0, 2, 3, 0, 1, 3, 3, 3, 3, 2, 1, 1, 3, 3, 3, 3])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_submit = submission_df[features].values\n",
    "\n",
    "y_submit_pred = clf.predict(X_submit)\n",
    "y_submit_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['HALV', 'HALV', 'LALV', 'HALV', 'HALV', 'LAHV', 'LALV', 'LALV',\n       'LALV', 'LAHV', 'LALV', 'LALV', 'LALV', 'LALV', 'LALV', 'LALV',\n       'HALV', 'LALV', 'LALV', 'LALV', 'HALV', 'LALV', 'HALV', 'LALV',\n       'HALV', 'HALV', 'LAHV', 'LAHV', 'LALV', 'HALV', 'LALV', 'HALV',\n       'HAHV', 'LAHV', 'HALV', 'HAHV', 'HALV', 'LALV', 'LALV', 'LALV',\n       'LAHV', 'LALV', 'HAHV', 'HAHV', 'LALV', 'LALV', 'HALV', 'HAHV',\n       'LAHV', 'HALV', 'LAHV', 'LALV', 'LALV', 'LALV', 'HAHV', 'LALV',\n       'LALV', 'HALV', 'LALV', 'LALV', 'LALV', 'LALV', 'HALV', 'HALV',\n       'HALV', 'HAHV', 'HALV', 'LALV', 'HALV', 'LAHV', 'LALV', 'LALV',\n       'HALV', 'HALV', 'LALV', 'HALV', 'HALV', 'HALV', 'LAHV', 'HALV',\n       'LALV', 'LALV', 'LALV', 'LAHV', 'LALV', 'LALV', 'LALV', 'HALV',\n       'HALV', 'HAHV', 'HALV', 'LALV', 'HAHV', 'LALV', 'LALV', 'LALV',\n       'LALV', 'LALV', 'LAHV', 'LAHV', 'HALV', 'LALV', 'LAHV', 'LAHV',\n       'HAHV', 'HAHV', 'HALV', 'LAHV', 'HALV', 'HALV', 'LALV', 'HAHV',\n       'LALV', 'HAHV', 'LAHV', 'LAHV', 'HALV', 'HALV', 'HALV', 'HALV',\n       'LALV', 'LALV', 'HALV', 'HAHV', 'LALV', 'HAHV', 'LALV', 'HALV',\n       'HAHV', 'HAHV', 'HAHV', 'LALV', 'LALV', 'LALV', 'LALV', 'HALV',\n       'LALV', 'HALV', 'HALV', 'LALV', 'LALV', 'LAHV', 'LAHV', 'HALV',\n       'LAHV', 'LALV', 'LALV', 'HALV', 'HALV', 'HAHV', 'HAHV', 'HALV',\n       'HALV', 'LALV', 'LALV', 'LALV', 'HALV', 'LALV', 'LAHV', 'HALV',\n       'LAHV', 'LALV', 'LALV', 'LALV', 'HALV', 'LALV', 'HALV', 'HALV',\n       'HALV', 'LALV', 'LALV', 'HALV', 'HALV', 'LALV', 'HALV', 'LALV',\n       'HALV', 'LALV', 'LALV', 'LALV', 'LALV', 'LALV', 'LALV', 'HALV',\n       'LAHV', 'LALV', 'LALV', 'HAHV', 'LALV', 'HALV', 'HALV', 'LALV',\n       'LALV', 'HAHV', 'LALV', 'LAHV', 'HALV', 'HALV', 'LALV', 'LALV',\n       'LALV', 'LALV', 'HALV', 'LALV', 'LAHV', 'HAHV', 'HAHV', 'LALV',\n       'LALV', 'LALV', 'LALV', 'HAHV', 'HAHV', 'HALV', 'LALV', 'LALV',\n       'HALV', 'LALV', 'HALV', 'HAHV', 'LALV', 'LAHV', 'LALV', 'LAHV',\n       'LALV', 'LALV', 'LALV', 'LALV', 'LAHV', 'HALV', 'HALV', 'LALV',\n       'LALV', 'HALV', 'LALV', 'HALV', 'HALV', 'HALV', 'HAHV', 'LALV',\n       'LALV', 'LALV', 'LALV', 'LALV', 'HALV', 'LALV', 'LALV', 'HALV',\n       'LAHV', 'HAHV', 'LALV', 'HALV', 'HALV', 'LAHV', 'HALV', 'LALV',\n       'HALV', 'HALV', 'HALV', 'LAHV', 'LAHV', 'LAHV', 'LALV', 'LAHV',\n       'LALV', 'LAHV', 'HALV', 'HAHV', 'HALV', 'LAHV', 'LAHV', 'LALV',\n       'LALV', 'HAHV', 'LALV', 'HAHV', 'LALV', 'LALV', 'LAHV', 'HALV',\n       'HAHV', 'LALV', 'LAHV', 'HAHV', 'LALV', 'LALV', 'LALV', 'HALV',\n       'LALV', 'HALV', 'LALV', 'LAHV', 'HAHV', 'LALV', 'LAHV', 'HAHV',\n       'LALV', 'HALV', 'LALV', 'LAHV', 'HALV', 'HAHV', 'HALV', 'LAHV',\n       'LAHV', 'HALV', 'LALV', 'HALV', 'LALV', 'HALV', 'HALV', 'HALV',\n       'HALV', 'HAHV', 'LALV', 'LALV', 'HALV', 'LAHV', 'HALV', 'LALV',\n       'LALV', 'LAHV', 'HALV', 'LALV', 'HAHV', 'LAHV', 'LALV', 'HALV',\n       'HALV', 'LALV', 'HAHV', 'LALV', 'LAHV', 'HALV', 'HALV', 'LALV',\n       'LAHV', 'HAHV', 'LALV', 'LALV', 'HALV', 'HALV', 'LALV', 'HALV',\n       'LALV', 'HALV', 'LALV', 'HALV', 'LALV', 'HALV', 'HALV', 'LALV',\n       'LALV', 'LAHV', 'LALV', 'LALV', 'HAHV', 'LALV', 'LALV', 'LALV',\n       'LALV', 'LAHV', 'HAHV', 'HALV', 'LAHV', 'HALV', 'LALV', 'LAHV',\n       'LALV', 'HALV', 'LALV', 'LALV', 'LALV', 'HALV', 'LAHV', 'HAHV',\n       'LALV', 'HALV', 'LAHV', 'HALV', 'LAHV', 'LAHV', 'HALV', 'LAHV',\n       'HAHV', 'LAHV', 'LALV', 'HALV', 'LAHV', 'LALV', 'LALV', 'LALV',\n       'LALV', 'LALV', 'LALV', 'LAHV', 'LAHV', 'HALV', 'LALV', 'LALV',\n       'LALV', 'HALV', 'HALV', 'LALV', 'HALV', 'LALV', 'HALV', 'LALV',\n       'LALV', 'LALV', 'HALV', 'LALV', 'HAHV', 'LALV', 'LALV', 'LALV',\n       'HAHV', 'HALV', 'LALV', 'HALV', 'LALV', 'LAHV', 'LALV', 'HALV',\n       'HALV', 'LALV', 'HALV', 'LALV', 'LALV', 'HALV', 'LALV', 'LALV',\n       'LALV', 'HALV', 'HALV', 'LAHV', 'HALV', 'HALV', 'HAHV', 'HAHV',\n       'LALV', 'HALV', 'HALV', 'HALV', 'LAHV', 'LALV', 'HALV', 'LALV',\n       'HALV', 'LALV', 'LALV', 'LALV', 'HALV', 'LALV', 'LALV', 'HALV',\n       'LALV', 'LALV', 'LALV', 'LAHV', 'LAHV', 'HALV', 'HAHV', 'LALV',\n       'LALV', 'LALV', 'HALV', 'LALV', 'HAHV', 'LAHV', 'LALV', 'LALV',\n       'LALV', 'LALV', 'HAHV', 'LALV', 'HALV', 'HALV', 'HAHV', 'HALV',\n       'HAHV', 'HALV', 'LAHV', 'LAHV', 'LALV', 'LAHV', 'LALV', 'HALV',\n       'HALV', 'HALV', 'HALV', 'LALV', 'LALV', 'HALV', 'HALV', 'LALV',\n       'LALV', 'LAHV', 'LALV', 'LALV', 'LAHV', 'LALV', 'HALV', 'LALV',\n       'HAHV', 'LALV', 'HAHV', 'LAHV', 'LALV', 'HAHV', 'HALV', 'LALV',\n       'LALV', 'LALV', 'LALV', 'LAHV', 'HALV', 'HALV', 'LALV', 'LALV',\n       'LALV', 'LALV'], dtype='<U4')"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_names = np.array([\"HAHV\", \"HALV\", \"LAHV\", \"LALV\"])\n",
    "t = dict_names[y_submit_pred]\n",
    "t\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "#make submission\n",
    "#id = submission_df['ID'].copy()\n",
    "predict = t\n",
    "submit_infor = {\n",
    "    \"Id\": submission_df['ID'],\n",
    "    \"Predicted\": predict\n",
    "}\n",
    "submit_final = pd.DataFrame(submit_infor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "#submission_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "           Id Predicted\n0    22Nj7cPW      HALV\n1    2A9QeqgD      HALV\n2    2AwkKhSz      LALV\n3    2SEMzqof      HALV\n4    2Sq686TD      HALV\n..        ...       ...\n283  KiSPDRg8      HALV\n284  bKHxJjeg      LALV\n285  3PNiPgms      LALV\n286  jyUDNewE      LALV\n287  gnSced5m      LALV\n\n[522 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22Nj7cPW</td>\n      <td>HALV</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2A9QeqgD</td>\n      <td>HALV</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2AwkKhSz</td>\n      <td>LALV</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2SEMzqof</td>\n      <td>HALV</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2Sq686TD</td>\n      <td>HALV</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>283</th>\n      <td>KiSPDRg8</td>\n      <td>HALV</td>\n    </tr>\n    <tr>\n      <th>284</th>\n      <td>bKHxJjeg</td>\n      <td>LALV</td>\n    </tr>\n    <tr>\n      <th>285</th>\n      <td>3PNiPgms</td>\n      <td>LALV</td>\n    </tr>\n    <tr>\n      <th>286</th>\n      <td>jyUDNewE</td>\n      <td>LALV</td>\n    </tr>\n    <tr>\n      <th>287</th>\n      <td>gnSced5m</td>\n      <td>LALV</td>\n    </tr>\n  </tbody>\n</table>\n<p>522 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_final\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "submit_final.to_csv('logs/submission_val_test.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at TabNet1.h5.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": "'TabNet1.h5.zip'"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.save_model('TabNet1.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}